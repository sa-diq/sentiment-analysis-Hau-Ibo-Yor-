{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Hausa, Igbo, and Yoruba Languages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.25.1 in /home/codespace/.local/lib/python3.10/site-packages (from -r requirements.txt (line 1)) (4.25.1)\n",
      "Requirement already satisfied: tensorflow==2.12.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r requirements.txt (line 2)) (2.12.0)\n",
      "Requirement already satisfied: torch>=1.12.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r requirements.txt (line 3)) (2.2.0)\n",
      "Requirement already satisfied: datasets>=2.9.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r requirements.txt (line 4)) (2.18.0)\n",
      "Requirement already satisfied: pandas>=1.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r requirements.txt (line 5)) (2.2.0)\n",
      "Requirement already satisfied: accelerate>=0.14.0 in /home/codespace/.local/lib/python3.10/site-packages (from -r requirements.txt (line 6)) (0.27.2)\n",
      "Requirement already satisfied: nltk==3.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from -r requirements.txt (line 7)) (3.5)\n",
      "Requirement already satisfied: xgboost>=2.0.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from -r requirements.txt (line 8)) (2.0.3)\n",
      "Requirement already satisfied: filelock in /home/codespace/.local/lib/python3.10/site-packages (from transformers==4.25.1->-r requirements.txt (line 1)) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.10.0 in /home/codespace/.local/lib/python3.10/site-packages (from transformers==4.25.1->-r requirements.txt (line 1)) (0.21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/codespace/.local/lib/python3.10/site-packages (from transformers==4.25.1->-r requirements.txt (line 1)) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/codespace/.local/lib/python3.10/site-packages (from transformers==4.25.1->-r requirements.txt (line 1)) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from transformers==4.25.1->-r requirements.txt (line 1)) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/codespace/.local/lib/python3.10/site-packages (from transformers==4.25.1->-r requirements.txt (line 1)) (2023.12.25)\n",
      "Requirement already satisfied: requests in /home/codespace/.local/lib/python3.10/site-packages (from transformers==4.25.1->-r requirements.txt (line 1)) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/codespace/.local/lib/python3.10/site-packages (from transformers==4.25.1->-r requirements.txt (line 1)) (0.13.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/codespace/.local/lib/python3.10/site-packages (from transformers==4.25.1->-r requirements.txt (line 1)) (4.66.2)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 2)) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 2)) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 2)) (23.5.26)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 2)) (0.4.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 2)) (0.2.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 2)) (1.62.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 2)) (3.10.0)\n",
      "Requirement already satisfied: jax>=0.3.15 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 2)) (0.4.25)\n",
      "Requirement already satisfied: keras<2.13,>=2.12.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 2)) (2.12.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 2)) (16.0.6)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 2)) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 2)) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 2)) (69.1.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 2)) (1.16.0)\n",
      "Requirement already satisfied: tensorboard<2.13,>=2.12 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 2)) (2.12.3)\n",
      "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 2)) (2.12.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 2)) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 2)) (4.10.0)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 2)) (1.14.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/codespace/.local/lib/python3.10/site-packages (from tensorflow==2.12.0->-r requirements.txt (line 2)) (0.36.0)\n",
      "Requirement already satisfied: click in /usr/local/python/3.10.13/lib/python3.10/site-packages (from nltk==3.5->-r requirements.txt (line 7)) (8.1.7)\n",
      "Requirement already satisfied: joblib in /home/codespace/.local/lib/python3.10/site-packages (from nltk==3.5->-r requirements.txt (line 7)) (1.3.2)\n",
      "Requirement already satisfied: sympy in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.12.0->-r requirements.txt (line 3)) (1.12)\n",
      "Requirement already satisfied: networkx in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.12.0->-r requirements.txt (line 3)) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.12.0->-r requirements.txt (line 3)) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.12.0->-r requirements.txt (line 3)) (2024.2.0)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.12.0->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.12.0->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.12.0->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.12.0->-r requirements.txt (line 3)) (8.9.2.26)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.12.0->-r requirements.txt (line 3)) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.12.0->-r requirements.txt (line 3)) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.12.0->-r requirements.txt (line 3)) (10.3.2.106)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.12.0->-r requirements.txt (line 3)) (11.4.5.107)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.12.0->-r requirements.txt (line 3)) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.12.0->-r requirements.txt (line 3)) (2.19.3)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.12.0->-r requirements.txt (line 3)) (12.1.105)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from torch>=1.12.0->-r requirements.txt (line 3)) (2.2.0)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/codespace/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.12.0->-r requirements.txt (line 3)) (12.3.101)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from datasets>=2.9.0->-r requirements.txt (line 4)) (15.0.0)\n",
      "Requirement already satisfied: pyarrow-hotfix in /home/codespace/.local/lib/python3.10/site-packages (from datasets>=2.9.0->-r requirements.txt (line 4)) (0.6)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from datasets>=2.9.0->-r requirements.txt (line 4)) (0.3.8)\n",
      "Requirement already satisfied: xxhash in /home/codespace/.local/lib/python3.10/site-packages (from datasets>=2.9.0->-r requirements.txt (line 4)) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /home/codespace/.local/lib/python3.10/site-packages (from datasets>=2.9.0->-r requirements.txt (line 4)) (0.70.16)\n",
      "Requirement already satisfied: aiohttp in /home/codespace/.local/lib/python3.10/site-packages (from datasets>=2.9.0->-r requirements.txt (line 4)) (3.9.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/codespace/.local/lib/python3.10/site-packages (from pandas>=1.5.0->-r requirements.txt (line 5)) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/codespace/.local/lib/python3.10/site-packages (from pandas>=1.5.0->-r requirements.txt (line 5)) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/codespace/.local/lib/python3.10/site-packages (from pandas>=1.5.0->-r requirements.txt (line 5)) (2023.4)\n",
      "Requirement already satisfied: psutil in /home/codespace/.local/lib/python3.10/site-packages (from accelerate>=0.14.0->-r requirements.txt (line 6)) (5.9.8)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/codespace/.local/lib/python3.10/site-packages (from accelerate>=0.14.0->-r requirements.txt (line 6)) (0.4.2)\n",
      "Requirement already satisfied: scipy in /home/codespace/.local/lib/python3.10/site-packages (from xgboost>=2.0.3->-r requirements.txt (line 8)) (1.12.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/codespace/.local/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.12.0->-r requirements.txt (line 2)) (0.42.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.9.0->-r requirements.txt (line 4)) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.9.0->-r requirements.txt (line 4)) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.9.0->-r requirements.txt (line 4)) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.9.0->-r requirements.txt (line 4)) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.9.0->-r requirements.txt (line 4)) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp->datasets>=2.9.0->-r requirements.txt (line 4)) (4.0.3)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jax>=0.3.15->tensorflow==2.12.0->-r requirements.txt (line 2)) (0.3.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers==4.25.1->-r requirements.txt (line 1)) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers==4.25.1->-r requirements.txt (line 1)) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers==4.25.1->-r requirements.txt (line 1)) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/codespace/.local/lib/python3.10/site-packages (from requests->transformers==4.25.1->-r requirements.txt (line 1)) (2024.2.2)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/codespace/.local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 2)) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /home/codespace/.local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 2)) (1.0.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/codespace/.local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 2)) (3.5.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /home/codespace/.local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 2)) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/codespace/.local/lib/python3.10/site-packages (from tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 2)) (3.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2->torch>=1.12.0->-r requirements.txt (line 3)) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/codespace/.local/lib/python3.10/site-packages (from sympy->torch>=1.12.0->-r requirements.txt (line 3)) (1.3.0)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 2)) (5.3.3)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/codespace/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 2)) (0.3.0)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/codespace/.local/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 2)) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/codespace/.local/lib/python3.10/site-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 2)) (1.3.1)\n",
      "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /home/codespace/.local/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 2)) (0.5.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/codespace/.local/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow==2.12.0->-r requirements.txt (line 2)) (3.2.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from plotly import graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly.figure_factory as ff\n",
    "import re\n",
    "\n",
    "\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, GRU,SimpleRNN\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import BatchNormalization\n",
    "#  from keras.utils import np_utils\n",
    "from sklearn import preprocessing, decomposition, model_selection, metrics, pipeline\n",
    "from keras.layers import GlobalMaxPooling1D, Conv1D, MaxPooling1D, Flatten, Bidirectional, SpatialDropout1D\n",
    "from keras.preprocessing import sequence, text\n",
    "from keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the datasets, stopwords, and lexicons\n",
    "languages = ['hausa', 'igbo', 'yoruba']\n",
    "train_data = {}\n",
    "dev_data = {}\n",
    "test_data = {}\n",
    "stopwords = {}\n",
    "lexicons = {}\n",
    "\n",
    "for lang in languages:\n",
    "    train_data[lang] = pd.read_csv(f'https://raw.githubusercontent.com/sa-diq/sentiment-analysis-Hau-Ibo-Yor-/main/data/{lang}_train.tsv', delimiter='\\t')\n",
    "    dev_data[lang] = pd.read_csv(f'https://raw.githubusercontent.com/sa-diq/sentiment-analysis-Hau-Ibo-Yor-/main/data/{lang}_dev.tsv', delimiter='\\t')\n",
    "    test_data[lang] = pd.read_csv(f'https://raw.githubusercontent.com/sa-diq/sentiment-analysis-Hau-Ibo-Yor-/main/data/{lang}_test.tsv', delimiter='\\t')\n",
    "    stopwords[lang] = pd.read_csv(f'https://raw.githubusercontent.com/sa-diq/sentiment-analysis-Hau-Ibo-Yor-/main/data/{lang}_stopwords.csv')\n",
    "    lexicons[lang] = pd.read_csv(f'https://raw.githubusercontent.com/sa-diq/sentiment-analysis-Hau-Ibo-Yor-/main/data/{lang}_lexicon.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hausa Train Data\n",
      "                                               tweet     label\n",
      "0  @user Da kudin da Arewa babu wani abin azo aga...  negative\n",
      "1  @user Kaga wani Adu ar Bandaüíîüò≠ wai a haka Shi ...  negative\n",
      "2  @user Sai haquri fa yan madrid daman kunce cha...  negative\n",
      "3  @user Hmmm yanzu kai kasan girman allah daxaka...  negative\n",
      "4  @user @user Wai gwamno nin Nigeria suna afa kw...  negative\n",
      "(14172, 2)\n",
      "\n",
      "\n",
      "Igbo Train Data\n",
      "                                               tweet     label\n",
      "0       Nna Ike Gwuru ooo. üòÇ https://t.co/NDS7juFBGd  negative\n",
      "1                 @user Chineke nna kezi mgbe ole???  negative\n",
      "2  Lol. Isi adirokwanu gi nma.. üòêüòíüòíüòí https://t.co...  negative\n",
      "3  @user haha. Fulani herdsmen. akpa amu gi retwe...  negative\n",
      "4  Nna ghetto di gi na aru biko!!! https://t.co/4...  negative\n",
      "(10192, 2)\n",
      "\n",
      "\n",
      "Yoruba Train Data\n",
      "                                               tweet     label\n",
      "0  √åw·ªç ik√∫ √≤p√≤n√∫ abarad√∫d√∫ w·ªç, o √≤ ·π£e √© 're o. O ...  negative\n",
      "1  Yor√πb√° nb√∫'y√†n ·π£√° \"\"\"\"\"\"\"\"..√†y√† wanle b√≠ √≤k√∫ √¨...  negative\n",
      "2  √íwe √†gb√† n√≠ \"\"\"\"\"\"\"\"·ªçm·ªçl·ªçÃÅm·ªç l√† √° r√°n n√≠·π£·∫πÃÅ √† ...  negative\n",
      "3  RT @user: @user asa kasa ti awon eyan ko ni od...  negative\n",
      "4  RT @user: Mo ≈Ñ r√≠ √†w·ªçn √®√©b√∫ k·ªçÃÄ·ªçÃÄkan. √Äti √†w·ªçn...  negative\n",
      "(8522, 2)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 rows of the hausa train data, yoruba train data, and igbo train data and their shapes\n",
    "print('Hausa Train Data')\n",
    "print(train_data['hausa'].head())\n",
    "print(train_data['hausa'].shape)\n",
    "print('\\n')\n",
    "print('Igbo Train Data')\n",
    "print(train_data['igbo'].head())\n",
    "print(train_data['igbo'].shape)\n",
    "print('\\n')\n",
    "print('Yoruba Train Data')\n",
    "print(train_data['yoruba'].head())\n",
    "print(train_data['yoruba'].shape)\n",
    "print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a function to clean the text data\n",
    "def clean_text_data(data):\n",
    "    if 'text' in data.columns:\n",
    "        data['text'] = data['text'].apply(clean_text)\n",
    "    return data\n",
    "def clean_text(text):\n",
    "    text = re.sub(r'@[A-Za-z0-9]+', '', text) #Removing @mentions\n",
    "    text = re.sub(r'#', '', text) # Removing '#' hash tag\n",
    "    text = re.sub(r'RT[\\s]+', '', text) # Removing RT\n",
    "    text = re.sub(r'https?:\\/\\/\\S+', '', text) # Removing the hyper link\n",
    "    text = re.sub(r'[^\\w\\s]', '', text) # Removing the punctuation\n",
    "    text = re.sub(r'\\d+', '', text) # Removing the digits\n",
    "    text = text.lower() # Converting the text to lower case\n",
    "    return text\n",
    "\n",
    "#  apply the clean_text function to the text data\n",
    "for lang in languages:\n",
    "    train_data[lang]['tweet'] = train_data[lang]['tweet'].apply(clean_text)\n",
    "    dev_data[lang]['tweet'] = dev_data[lang]['tweet'].apply(clean_text)\n",
    "    test_data[lang]['tweet'] = test_data[lang]['tweet'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hausa Train Data\n",
      "                                               tweet     label\n",
      "0   da kudin da arewa babu wani abin azo agani da...  negative\n",
      "1   kaga wani adu ar banda wai a haka shi ne shug...  negative\n",
      "2   sai haquri fa yan madrid daman kunce champion...  negative\n",
      "3   hmmm yanzu kai kasan girman allah daxakace mu...  negative\n",
      "4        wai gwamno nin nigeria suna afa kwayoyi ko   negative\n"
     ]
    }
   ],
   "source": [
    "# Print the first 5 rows of the hausa train data\n",
    "print('Hausa Train Data')\n",
    "print(train_data['hausa'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yoruba Train Data\n",
      "                                               tweet     label\n",
      "0  da kudin da arewa babu wani abin azo agani da ...  negative\n",
      "1  kaga wani adu ar banda wai a haka shi ne shuga...  negative\n",
      "2  sai haquri fa yan madrid daman kunce champion ...  negative\n",
      "3  hmmm yanzu kai kasan girman allah daxakace muk...  negative\n",
      "4         wai gwamno nin nigeria suna afa kwayoyi ko  negative\n"
     ]
    }
   ],
   "source": [
    "# Remove the stopwords from the train, test, dev data for the three languages\n",
    "def remove_stopwords(data, stopwords):\n",
    "    data['tweet'] = data['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stopwords)]))\n",
    "    return data\n",
    "\n",
    "for lang in languages:\n",
    "    train_data[lang] = remove_stopwords(train_data[lang], stopwords[lang])\n",
    "    dev_data[lang] = remove_stopwords(dev_data[lang], stopwords[lang])\n",
    "    test_data[lang] = remove_stopwords(test_data[lang], stopwords[lang])\n",
    "\n",
    "# Print the first 5 rows of the hausa train data\n",
    "print('Yoruba Train Data')\n",
    "print(train_data['hausa'].head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to calculate the sentiment score\n",
    "def calculate_sentiment_score(text, lexicon_dict):\n",
    "    score = 0\n",
    "    words = text.split()\n",
    "    for word in words:\n",
    "        score += lexicon_dict.get(word, 0)\n",
    "    return score\n",
    "\n",
    "def process_data(df, lexicon):\n",
    "    lexicon_dict = lexicon.set_index('human')['label'].map({'positive': 1, 'negative': -1}).to_dict()\n",
    "    df[\"sentiment_score\"] = df[\"tweet\"].apply(calculate_sentiment_score, args=(lexicon_dict,))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the process_data function to the train, test, dev data for the three languages   \n",
    "for lang in languages:\n",
    "    train_data[lang] = process_data(train_data[lang], lexicons[lang])\n",
    "    dev_data[lang] = process_data(dev_data[lang], lexicons[lang])\n",
    "    test_data[lang] = process_data(test_data[lang], lexicons[lang])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hausa Train Data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>da kudin da arewa babu wani abin azo agani da ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kaga wani adu ar banda wai a haka shi ne shuga...</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sai haquri fa yan madrid daman kunce champion ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hmmm yanzu kai kasan girman allah daxakace muk...</td>\n",
       "      <td>negative</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wai gwamno nin nigeria suna afa kwayoyi ko</td>\n",
       "      <td>negative</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet     label  \\\n",
       "0  da kudin da arewa babu wani abin azo agani da ...  negative   \n",
       "1  kaga wani adu ar banda wai a haka shi ne shuga...  negative   \n",
       "2  sai haquri fa yan madrid daman kunce champion ...  negative   \n",
       "3  hmmm yanzu kai kasan girman allah daxakace muk...  negative   \n",
       "4         wai gwamno nin nigeria suna afa kwayoyi ko  negative   \n",
       "\n",
       "   sentiment_score  \n",
       "0               -1  \n",
       "1                1  \n",
       "2                1  \n",
       "3               -5  \n",
       "4               -1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 5 rows of the hausa train data\n",
    "print('Hausa Train Data')\n",
    "train_data['hausa'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perfrom label encoding on the label column\n",
    "le = preprocessing.LabelEncoder()\n",
    "for lang in languages:\n",
    "    train_data[lang]['label'] = le.fit_transform(train_data[lang]['label'])\n",
    "    dev_data[lang]['label'] = le.transform(dev_data[lang]['label'])\n",
    "    test_data[lang]['label'] = le.transform(test_data[lang]['label'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hausa Train Data\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>da kudin da arewa babu wani abin azo agani da ...</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>kaga wani adu ar banda wai a haka shi ne shuga...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>sai haquri fa yan madrid daman kunce champion ...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hmmm yanzu kai kasan girman allah daxakace muk...</td>\n",
       "      <td>0</td>\n",
       "      <td>-5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>wai gwamno nin nigeria suna afa kwayoyi ko</td>\n",
       "      <td>0</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet  label  sentiment_score\n",
       "0  da kudin da arewa babu wani abin azo agani da ...      0               -1\n",
       "1  kaga wani adu ar banda wai a haka shi ne shuga...      0                1\n",
       "2  sai haquri fa yan madrid daman kunce champion ...      0                1\n",
       "3  hmmm yanzu kai kasan girman allah daxakace muk...      0               -5\n",
       "4         wai gwamno nin nigeria suna afa kwayoyi ko      0               -1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print the first 5 rows of the hausa train data\n",
    "print('Hausa Train Data')\n",
    "train_data['hausa'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install nltk\n",
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# def tokenize_text(text):\n",
    "#     return nltk.word_tokenize(text)\n",
    "\n",
    "# # Apply the tokenize_text function to the train, test, dev data for the three languages\n",
    "# for lang in languages:\n",
    "#     train_data[lang]['tweet'] = train_data[lang]['tweet'].apply(tokenize_text)\n",
    "#     dev_data[lang]['tweet'] = dev_data[lang]['tweet'].apply(tokenize_text)\n",
    "#     test_data[lang]['tweet'] = test_data[lang]['tweet'].apply(tokenize_text)\n",
    "    \n",
    "# # print the first 5 rows of the hausa train data\n",
    "# print('Hausa Train Data')\n",
    "# train_data['hausa'].head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_bow for hausa: (14172, 21786)\n",
      "Shape of X_dev_bow for hausa: (2677, 21786)\n",
      "Shape of X_test_bow for hausa: (5303, 21786)\n",
      "Shape of X_train_bow for igbo: (10192, 15502)\n",
      "Shape of X_dev_bow for igbo: (1841, 15502)\n",
      "Shape of X_test_bow for igbo: (3682, 15502)\n",
      "Shape of X_train_bow for yoruba: (8522, 22316)\n",
      "Shape of X_dev_bow for yoruba: (2090, 22316)\n",
      "Shape of X_test_bow for yoruba: (4515, 22316)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed text data to obtain BoW features\n",
    "X_train_bow = {}\n",
    "X_dev_bow = {}\n",
    "X_test_bow = {}\n",
    "\n",
    "for lang in languages:\n",
    "    X_train_bow[lang] = vectorizer.fit_transform(train_data[lang]['tweet'])\n",
    "    X_dev_bow[lang] = vectorizer.transform(dev_data[lang]['tweet'])\n",
    "    X_test_bow[lang] = vectorizer.transform(test_data[lang]['tweet'])\n",
    "\n",
    "# Display the shape of the obtained feature matrices\n",
    "for lang in languages:\n",
    "    print(f\"Shape of X_train_bow for {lang}:\", X_train_bow[lang].shape)\n",
    "    print(f\"Shape of X_dev_bow for {lang}:\", X_dev_bow[lang].shape)\n",
    "    print(f\"Shape of X_test_bow for {lang}:\", X_test_bow[lang].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X_train_tfidf for hausa: (14172, 21786)\n",
      "Shape of X_dev_tfidf for hausa: (2677, 21786)\n",
      "Shape of X_test_tfidf for hausa: (5303, 21786)\n",
      "Shape of X_train_tfidf for igbo: (10192, 15502)\n",
      "Shape of X_dev_tfidf for igbo: (1841, 15502)\n",
      "Shape of X_test_tfidf for igbo: (3682, 15502)\n",
      "Shape of X_train_tfidf for yoruba: (8522, 22316)\n",
      "Shape of X_dev_tfidf for yoruba: (2090, 22316)\n",
      "Shape of X_test_tfidf for yoruba: (4515, 22316)\n"
     ]
    }
   ],
   "source": [
    "# Initialize TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the preprocessed text data to obtain tf-idf features\n",
    "X_train_tfidf = {}\n",
    "X_dev_tfidf = {}\n",
    "X_test_tfidf = {}\n",
    "\n",
    "for lang in languages:\n",
    "    X_train_tfidf[lang] = tfidf_vectorizer.fit_transform(train_data[lang]['tweet'])\n",
    "    X_dev_tfidf[lang] = tfidf_vectorizer.transform(dev_data[lang]['tweet'])\n",
    "    X_test_tfidf[lang] = tfidf_vectorizer.transform(test_data[lang]['tweet'])\n",
    "\n",
    "# Display the shape of the obtained feature matrices\n",
    "for lang in languages:\n",
    "    print(f\"Shape of X_train_tfidf for {lang}:\", X_train_tfidf[lang].shape)\n",
    "    print(f\"Shape of X_dev_tfidf for {lang}:\", X_dev_tfidf[lang].shape)\n",
    "    print(f\"Shape of X_test_tfidf for {lang}:\", X_test_tfidf[lang].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a function to include the sentiment score as a feature in the feature matrix\n",
    "# def add_sentiment_score(X, data):\n",
    "#     sentiment_score = data['sentiment_score'].values\n",
    "#     if X.shape[0] != sentiment_score.shape[0]:\n",
    "#         raise ValueError(f\"Shape mismatch: X has shape {X.shape} but sentiment_score has shape {sentiment_score.shape}\")\n",
    "#     return np.concatenate((X.toarray(), sentiment_score.reshape(-1, 1)), axis=1)\n",
    "\n",
    "# # Apply the add_sentiment_score function to the tf-idf feature matrices for the three languages\n",
    "# X_train_tfidf_sentiment = {}\n",
    "# X_dev_tfidf_sentiment = {}\n",
    "# X_test_tfidf_sentiment = {}\n",
    "\n",
    "# for lang in languages:\n",
    "#     X_train_tfidf_sentiment[lang] = add_sentiment_score(X_train_tfidf[lang], train_data[lang])\n",
    "#     X_dev_tfidf_sentiment[lang] = add_sentiment_score(X_dev_tfidf[lang], dev_data[lang])\n",
    "#     X_test_tfidf_sentiment[lang] = add_sentiment_score(X_test_tfidf[lang], test_data[lang])\n",
    "    \n",
    "# # Display the shape of the obtained feature matrices\n",
    "# for lang in languages:\n",
    "#     print(f\"Shape of X_train_tfidf_sentiment for {lang}:\", X_train_tfidf_sentiment[lang].shape)\n",
    "#     print(f\"Shape of X_dev_tfidf_sentiment for {lang}:\", X_dev_tfidf_sentiment[lang].shape)\n",
    "#     print(f\"Shape of X_test_tfidf_sentiment for {lang}:\", X_test_tfidf_sentiment[lang].shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to train a logistic regression model with the given feature matrix and labels\n",
    "def train_logistic_regression(X, y):\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "# Create a function to train a naive bayes model with the given feature matrix and labels\n",
    "def train_naive_bayes(X, y):\n",
    "    model = MultinomialNB()\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "# Create a function to train a support vector machine model with the given feature matrix and labels\n",
    "def train_svm(X, y):\n",
    "    model = SVC()\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "# Create a function to make predictions using the given model and feature matrix\n",
    "def predict(model, X):\n",
    "    return model.predict(X)\n",
    "\n",
    "# Create a function to evaluate the given model using the given feature matrix and labels\n",
    "def evaluate_model(model, X, y):\n",
    "    y_pred = predict(model, X)\n",
    "    return accuracy_score(y, y_pred)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of logistic regression model for hausa: 0.7411281285020546\n",
      "Accuracy of logistic regression model for igbo: 0.77729494839761\n",
      "Accuracy of logistic regression model for yoruba: 0.7272727272727273\n",
      "Accuracy of naive bayes model for hausa: 0.711243929772133\n",
      "Accuracy of naive bayes model for igbo: 0.7278652906029331\n",
      "Accuracy of naive bayes model for yoruba: 0.6588516746411484\n"
     ]
    }
   ],
   "source": [
    "# Train logistic regression models using the tf-idf feature matrices for the three languages\n",
    "logistic_regression_models = {}\n",
    "for lang in languages:\n",
    "    logistic_regression_models[lang] = train_logistic_regression(X_train_tfidf[lang], train_data[lang]['label'])\n",
    "    \n",
    "# Evaluate the logistic regression models using the dev data\n",
    "logistic_regression_scores = {}\n",
    "for lang in languages:\n",
    "    logistic_regression_scores[lang] = evaluate_model(logistic_regression_models[lang], X_dev_tfidf[lang], dev_data[lang]['label'])\n",
    "    print(f\"Accuracy of logistic regression model for {lang}:\", logistic_regression_scores[lang])\n",
    "\n",
    "# Train naive bayes models using the tf-idf feature matrices for the three languages\n",
    "naive_bayes_models = {}\n",
    "for lang in languages:\n",
    "    naive_bayes_models[lang] = train_naive_bayes(X_train_tfidf[lang], train_data[lang]['label'])\n",
    "\n",
    "# Evaluate the naive bayes models using the dev data\n",
    "naive_bayes_scores = {}\n",
    "for lang in languages:\n",
    "    naive_bayes_scores[lang] = evaluate_model(naive_bayes_models[lang], X_dev_tfidf[lang], dev_data[lang]['label'])\n",
    "    print(f\"Accuracy of naive bayes model for {lang}:\", naive_bayes_scores[lang])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for logistic regression model for hausa:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.66      0.69      1759\n",
      "           1       0.66      0.76      0.71      1789\n",
      "           2       0.85      0.80      0.82      1755\n",
      "\n",
      "    accuracy                           0.74      5303\n",
      "   macro avg       0.75      0.74      0.74      5303\n",
      "weighted avg       0.75      0.74      0.74      5303\n",
      "\n",
      "Classification report for logistic regression model for igbo:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.61      0.70       943\n",
      "           1       0.72      0.88      0.79      1621\n",
      "           2       0.87      0.76      0.81      1118\n",
      "\n",
      "    accuracy                           0.78      3682\n",
      "   macro avg       0.80      0.75      0.77      3682\n",
      "weighted avg       0.79      0.78      0.77      3682\n",
      "\n",
      "Classification report for logistic regression model for yoruba:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.54      0.60       981\n",
      "           1       0.72      0.72      0.72      1616\n",
      "           2       0.76      0.84      0.80      1918\n",
      "\n",
      "    accuracy                           0.73      4515\n",
      "   macro avg       0.72      0.70      0.70      4515\n",
      "weighted avg       0.73      0.73      0.73      4515\n",
      "\n",
      "Classification report for naive bayes model for hausa:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.72      0.70      1759\n",
      "           1       0.68      0.66      0.67      1789\n",
      "           2       0.82      0.79      0.80      1755\n",
      "\n",
      "    accuracy                           0.72      5303\n",
      "   macro avg       0.72      0.72      0.72      5303\n",
      "weighted avg       0.72      0.72      0.72      5303\n",
      "\n",
      "Classification report for naive bayes model for igbo:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.41      0.57       943\n",
      "           1       0.64      0.96      0.77      1621\n",
      "           2       0.89      0.67      0.77      1118\n",
      "\n",
      "    accuracy                           0.73      3682\n",
      "   macro avg       0.82      0.68      0.70      3682\n",
      "weighted avg       0.79      0.73      0.72      3682\n",
      "\n",
      "Classification report for naive bayes model for yoruba:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.11      0.20       981\n",
      "           1       0.65      0.73      0.69      1616\n",
      "           2       0.65      0.88      0.75      1918\n",
      "\n",
      "    accuracy                           0.66      4515\n",
      "   macro avg       0.73      0.57      0.55      4515\n",
      "weighted avg       0.70      0.66      0.61      4515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the logistic regression models using the test data and print the classification report\n",
    "logistic_regression_predictions = {}\n",
    "for lang in languages:\n",
    "    logistic_regression_predictions[lang] = predict(logistic_regression_models[lang], X_test_tfidf[lang])\n",
    "    print(f\"Classification report for logistic regression model for {lang}:\\n\", classification_report(test_data[lang]['label'], logistic_regression_predictions[lang]))\n",
    "\n",
    "# Test the naive bayes models using the test data and print the classification report\n",
    "naive_bayes_predictions = {}\n",
    "for lang in languages:\n",
    "    naive_bayes_predictions[lang] = predict(naive_bayes_models[lang], X_test_tfidf[lang])\n",
    "    print(f\"Classification report for naive bayes model for {lang}:\\n\", classification_report(test_data[lang]['label'], naive_bayes_predictions[lang]))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of tuned logistic regression model for hausa: 0.7373926036608144\n",
      "Accuracy of tuned logistic regression model for igbo: 0.7800108636610538\n",
      "Accuracy of tuned logistic regression model for yoruba: 0.7421052631578947\n"
     ]
    }
   ],
   "source": [
    "#  Create a function to tune the hyperparameters of the logistic regression model\n",
    "def tune_logistic_regression(X, y):\n",
    "    model = LogisticRegression(max_iter=1000)\n",
    "    param_grid = {'C': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "    grid = GridSearchCV(model, param_grid, cv=5)\n",
    "    grid.fit(X, y)\n",
    "    return grid.best_estimator_\n",
    "\n",
    "# Create a function to tune the hyperparameters of the naive bayes model\n",
    "def tune_naive_bayes(X, y):\n",
    "    model = MultinomialNB()\n",
    "    param_grid = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "    grid = GridSearchCV(model, param_grid, cv=5)\n",
    "    grid.fit(X, y)\n",
    "    return grid.best_estimator_\n",
    "\n",
    "# Tune the hyperparameters of the logistic regression models using the tf-idf feature matrices for the three languages\n",
    "tuned_logistic_regression_models = {}\n",
    "for lang in languages:\n",
    "    tuned_logistic_regression_models[lang] = tune_logistic_regression(X_train_tfidf[lang], train_data[lang]['label'])\n",
    "    \n",
    "# Evaluate the tuned logistic regression models using the dev data\n",
    "tuned_logistic_regression_scores = {}\n",
    "for lang in languages:\n",
    "    tuned_logistic_regression_scores[lang] = evaluate_model(tuned_logistic_regression_models[lang], X_dev_tfidf[lang], dev_data[lang]['label'])\n",
    "    print(f\"Accuracy of tuned logistic regression model for {lang}:\", tuned_logistic_regression_scores[lang])\n",
    "\n",
    "# Tune the hyperparameters of the naive bayes models using the tf-idf feature matrices for the three languages\n",
    "tuned_naive_bayes_models = {}\n",
    "for lang in languages:\n",
    "    tuned_naive_bayes_models[lang] = tune_naive_bayes(X_train_tfidf[lang], train_data[lang]['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for tuned logistic regression model for hausa:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.68      0.70      1759\n",
      "           1       0.68      0.74      0.71      1789\n",
      "           2       0.84      0.82      0.83      1755\n",
      "\n",
      "    accuracy                           0.75      5303\n",
      "   macro avg       0.75      0.75      0.75      5303\n",
      "weighted avg       0.75      0.75      0.75      5303\n",
      "\n",
      "Classification report for tuned logistic regression model for igbo:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.68      0.71       943\n",
      "           1       0.75      0.82      0.78      1621\n",
      "           2       0.83      0.78      0.81      1118\n",
      "\n",
      "    accuracy                           0.77      3682\n",
      "   macro avg       0.77      0.76      0.77      3682\n",
      "weighted avg       0.77      0.77      0.77      3682\n",
      "\n",
      "Classification report for tuned logistic regression model for yoruba:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.60      0.62       981\n",
      "           1       0.72      0.70      0.71      1616\n",
      "           2       0.77      0.82      0.80      1918\n",
      "\n",
      "    accuracy                           0.73      4515\n",
      "   macro avg       0.71      0.71      0.71      4515\n",
      "weighted avg       0.73      0.73      0.73      4515\n",
      "\n",
      "Classification report for tuned naive bayes model for hausa:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.67      0.72      0.70      1759\n",
      "           1       0.68      0.66      0.67      1789\n",
      "           2       0.82      0.79      0.80      1755\n",
      "\n",
      "    accuracy                           0.72      5303\n",
      "   macro avg       0.72      0.72      0.72      5303\n",
      "weighted avg       0.72      0.72      0.72      5303\n",
      "\n",
      "Classification report for tuned naive bayes model for igbo:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.58      0.66       943\n",
      "           1       0.69      0.84      0.75      1621\n",
      "           2       0.81      0.72      0.76      1118\n",
      "\n",
      "    accuracy                           0.74      3682\n",
      "   macro avg       0.75      0.71      0.73      3682\n",
      "weighted avg       0.74      0.74      0.73      3682\n",
      "\n",
      "Classification report for tuned naive bayes model for yoruba:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.47      0.57       981\n",
      "           1       0.69      0.74      0.72      1616\n",
      "           2       0.73      0.82      0.77      1918\n",
      "\n",
      "    accuracy                           0.71      4515\n",
      "   macro avg       0.71      0.68      0.68      4515\n",
      "weighted avg       0.71      0.71      0.71      4515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test the tuned logistic regression models using the test data and print the classification report\n",
    "tuned_logistic_regression_predictions = {}\n",
    "for lang in languages:\n",
    "    tuned_logistic_regression_predictions[lang] = predict(tuned_logistic_regression_models[lang], X_test_tfidf[lang])\n",
    "    print(f\"Classification report for tuned logistic regression model for {lang}:\\n\", classification_report(test_data[lang]['label'], tuned_logistic_regression_predictions[lang]))\n",
    "    \n",
    "# Test the tuned naive bayes models using the test data and print the classification report\n",
    "tuned_naive_bayes_predictions = {}\n",
    "for lang in languages:\n",
    "    tuned_naive_bayes_predictions[lang] = predict(tuned_naive_bayes_models[lang], X_test_tfidf[lang])\n",
    "    print(f\"Classification report for tuned naive bayes model for {lang}:\\n\", classification_report(test_data[lang]['label'], tuned_naive_bayes_predictions[lang]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification report for Random Forest model for hausa:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.58      0.65      1759\n",
      "           1       0.63      0.79      0.70      1789\n",
      "           2       0.83      0.79      0.81      1755\n",
      "\n",
      "    accuracy                           0.72      5303\n",
      "   macro avg       0.73      0.72      0.72      5303\n",
      "weighted avg       0.73      0.72      0.72      5303\n",
      "\n",
      "Classification report for Random Forest model for igbo:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.77      0.59      0.67       943\n",
      "           1       0.70      0.87      0.77      1621\n",
      "           2       0.88      0.74      0.80      1118\n",
      "\n",
      "    accuracy                           0.76      3682\n",
      "   macro avg       0.78      0.73      0.75      3682\n",
      "weighted avg       0.77      0.76      0.76      3682\n",
      "\n",
      "Classification report for Random Forest model for yoruba:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.30      0.43       981\n",
      "           1       0.64      0.75      0.69      1616\n",
      "           2       0.71      0.82      0.76      1918\n",
      "\n",
      "    accuracy                           0.68      4515\n",
      "   macro avg       0.70      0.62      0.63      4515\n",
      "weighted avg       0.69      0.68      0.66      4515\n",
      "\n",
      "Classification report for Gradient Boosting model for hausa:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.43      0.55      1759\n",
      "           1       0.56      0.87      0.69      1789\n",
      "           2       0.82      0.72      0.76      1755\n",
      "\n",
      "    accuracy                           0.67      5303\n",
      "   macro avg       0.71      0.67      0.66      5303\n",
      "weighted avg       0.71      0.67      0.66      5303\n",
      "\n",
      "Classification report for Gradient Boosting model for igbo:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.47      0.59       943\n",
      "           1       0.65      0.92      0.76      1621\n",
      "           2       0.88      0.63      0.74      1118\n",
      "\n",
      "    accuracy                           0.72      3682\n",
      "   macro avg       0.77      0.68      0.69      3682\n",
      "weighted avg       0.75      0.72      0.71      3682\n",
      "\n",
      "Classification report for Gradient Boosting model for yoruba:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.32      0.43       981\n",
      "           1       0.64      0.63      0.64      1616\n",
      "           2       0.64      0.82      0.72      1918\n",
      "\n",
      "    accuracy                           0.64      4515\n",
      "   macro avg       0.64      0.59      0.59      4515\n",
      "weighted avg       0.64      0.64      0.62      4515\n",
      "\n",
      "Classification report for XGBoost model for hausa:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.56      0.64      1759\n",
      "           1       0.61      0.83      0.70      1789\n",
      "           2       0.85      0.77      0.81      1755\n",
      "\n",
      "    accuracy                           0.72      5303\n",
      "   macro avg       0.74      0.72      0.72      5303\n",
      "weighted avg       0.74      0.72      0.72      5303\n",
      "\n",
      "Classification report for XGBoost model for igbo:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.59      0.68       943\n",
      "           1       0.70      0.88      0.78      1621\n",
      "           2       0.87      0.75      0.80      1118\n",
      "\n",
      "    accuracy                           0.76      3682\n",
      "   macro avg       0.79      0.74      0.75      3682\n",
      "weighted avg       0.78      0.76      0.76      3682\n",
      "\n",
      "Classification report for XGBoost model for yoruba:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.63      0.43      0.51       981\n",
      "           1       0.65      0.73      0.69      1616\n",
      "           2       0.73      0.78      0.76      1918\n",
      "\n",
      "    accuracy                           0.69      4515\n",
      "   macro avg       0.67      0.65      0.65      4515\n",
      "weighted avg       0.68      0.69      0.68      4515\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "# Create a function to train a Random Forest model with the given feature matrix and labels\n",
    "def train_random_forest(X, y):\n",
    "    model = RandomForestClassifier()\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "# Train Random Forest models using the tf-idf feature matrices for the three languages\n",
    "random_forest_models = {}\n",
    "for lang in languages:\n",
    "    random_forest_models[lang] = train_random_forest(X_train_tfidf[lang], train_data[lang]['label'])\n",
    "\n",
    "# Test the Random Forest models using the test data and print the classification report\n",
    "random_forest_predictions = {}\n",
    "for lang in languages:\n",
    "    random_forest_predictions[lang] = predict(random_forest_models[lang], X_test_tfidf[lang])\n",
    "    print(f\"Classification report for Random Forest model for {lang}:\\n\", classification_report(test_data[lang]['label'], random_forest_predictions[lang]))\n",
    "\n",
    "# Create a function to train a Gradient Boosting model with the given feature matrix and labels\n",
    "def train_gradient_boosting(X, y):\n",
    "    model = GradientBoostingClassifier()\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "# Train Gradient Boosting models using the tf-idf feature matrices for the three languages\n",
    "gradient_boosting_models = {}\n",
    "for lang in languages:\n",
    "    gradient_boosting_models[lang] = train_gradient_boosting(X_train_tfidf[lang], train_data[lang]['label'])\n",
    "\n",
    "# Test the Gradient Boosting models using the test data and print the classification report\n",
    "gradient_boosting_predictions = {}\n",
    "for lang in languages:\n",
    "    gradient_boosting_predictions[lang] = predict(gradient_boosting_models[lang], X_test_tfidf[lang])\n",
    "    print(f\"Classification report for Gradient Boosting model for {lang}:\\n\", classification_report(test_data[lang]['label'], gradient_boosting_predictions[lang]))\n",
    "    \n",
    "# Create a function to train a XGBoost model with the given feature matrix and labels\n",
    "def train_xgboost(X, y):\n",
    "    model = XGBClassifier()\n",
    "    model.fit(X, y)\n",
    "    return model\n",
    "\n",
    "# Train XGBoost models using the tf-idf feature matrices for the three languages\n",
    "xgboost_models = {}\n",
    "for lang in languages:\n",
    "    xgboost_models[lang] = train_xgboost(X_train_tfidf[lang], train_data[lang]['label'])\n",
    "\n",
    "# Test the XGBoost models using the test data and print the classification report\n",
    "xgboost_predictions = {}\n",
    "for lang in languages:\n",
    "    xgboost_predictions[lang] = predict(xgboost_models[lang], X_test_tfidf[lang])\n",
    "    print(f\"Classification report for XGBoost model for {lang}:\\n\", classification_report(test_data[lang]['label'], xgboost_predictions[lang]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function to tokenize the text data using the Tokenizer class\n",
    "def tokenize_text(data, max_words):\n",
    "    tokenizer = Tokenizer(num_words=max_words)\n",
    "    tokenizer.fit_on_texts(data)\n",
    "    return tokenizer\n",
    "\n",
    "# Tokenize the text data using the Tokenizer class\n",
    "max_words = 1000\n",
    "tokenizers = {}\n",
    "for lang in languages:\n",
    "    tokenizers[lang] = tokenize_text(train_data[lang]['tweet'], max_words)\n",
    "\n",
    "# Create a function to convert the text data to sequences using the Tokenizer class\n",
    "def convert_to_sequences(tokenizer, data):\n",
    "    return tokenizer.texts_to_sequences(data)\n",
    "\n",
    "# Convert the text data to sequences using the Tokenizer class\n",
    "X_train_sequences = {}\n",
    "X_dev_sequences = {}\n",
    "X_test_sequences = {}\n",
    "\n",
    "for lang in languages:\n",
    "    X_train_sequences[lang] = convert_to_sequences(tokenizers[lang], train_data[lang]['tweet'])\n",
    "    X_dev_sequences[lang] = convert_to_sequences(tokenizers[lang], dev_data[lang]['tweet'])\n",
    "    X_test_sequences[lang] = convert_to_sequences(tokenizers[lang], test_data[lang]['tweet'])\n",
    "\n",
    "# Add padding to the sequences\n",
    "maxlen = 100\n",
    "X_train_sequences_padded = {}\n",
    "X_dev_sequences_padded = {}\n",
    "X_test_sequences_padded = {}\n",
    "\n",
    "for lang in languages:\n",
    "    X_train_sequences_padded[lang] = pad_sequences(X_train_sequences[lang], maxlen=maxlen)\n",
    "    X_dev_sequences_padded[lang] = pad_sequences(X_dev_sequences[lang], maxlen=maxlen)\n",
    "    X_test_sequences_padded[lang] = pad_sequences(X_test_sequences[lang], maxlen=maxlen)\n",
    "\n",
    "\n",
    "# Create a function to one-hot encode the labels\n",
    "def one_hot_encode_labels(y, num_classes):\n",
    "    return tf.keras.utils.to_categorical(y, num_classes)\n",
    "\n",
    "# One-hot encode the labels\n",
    "num_classes = 3\n",
    "y_train_one_hot = {}\n",
    "y_dev_one_hot = {}\n",
    "y_test_one_hot = {}\n",
    "\n",
    "for lang in languages:\n",
    "    y_train_one_hot[lang] = one_hot_encode_labels(train_data[lang]['label'], num_classes)\n",
    "    y_dev_one_hot[lang] = one_hot_encode_labels(dev_data[lang]['label'], num_classes)\n",
    "    y_test_one_hot[lang] = one_hot_encode_labels(test_data[lang]['label'], num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "355/355 [==============================] - 6s 14ms/step - loss: 0.8991 - accuracy: 0.5583 - val_loss: 1.2197 - val_accuracy: 0.5513\n",
      "Epoch 2/10\n",
      "355/355 [==============================] - 5s 13ms/step - loss: 0.6662 - accuracy: 0.7096 - val_loss: 0.9206 - val_accuracy: 0.6402\n",
      "Epoch 3/10\n",
      "355/355 [==============================] - 5s 13ms/step - loss: 0.5813 - accuracy: 0.7553 - val_loss: 0.9399 - val_accuracy: 0.6437\n",
      "Epoch 4/10\n",
      "355/355 [==============================] - 5s 13ms/step - loss: 0.5236 - accuracy: 0.7849 - val_loss: 0.7850 - val_accuracy: 0.7125\n",
      "Epoch 5/10\n",
      "355/355 [==============================] - 5s 13ms/step - loss: 0.4823 - accuracy: 0.8046 - val_loss: 0.9464 - val_accuracy: 0.6600\n",
      "Epoch 6/10\n",
      "355/355 [==============================] - 5s 13ms/step - loss: 0.4177 - accuracy: 0.8322 - val_loss: 1.0477 - val_accuracy: 0.6557\n",
      "Epoch 7/10\n",
      "355/355 [==============================] - 5s 13ms/step - loss: 0.3636 - accuracy: 0.8599 - val_loss: 1.0191 - val_accuracy: 0.6536\n",
      "Epoch 8/10\n",
      "355/355 [==============================] - 5s 13ms/step - loss: 0.3253 - accuracy: 0.8747 - val_loss: 1.2400 - val_accuracy: 0.6466\n",
      "Epoch 9/10\n",
      "355/355 [==============================] - 5s 14ms/step - loss: 0.2904 - accuracy: 0.8918 - val_loss: 1.1936 - val_accuracy: 0.6511\n",
      "Epoch 10/10\n",
      "355/355 [==============================] - 5s 13ms/step - loss: 0.2618 - accuracy: 0.9024 - val_loss: 1.5320 - val_accuracy: 0.6078\n",
      "Epoch 1/10\n",
      "255/255 [==============================] - 4s 14ms/step - loss: 0.9006 - accuracy: 0.5954 - val_loss: 1.9499 - val_accuracy: 4.9044e-04\n",
      "Epoch 2/10\n",
      "255/255 [==============================] - 4s 14ms/step - loss: 0.7235 - accuracy: 0.7071 - val_loss: 1.6507 - val_accuracy: 0.1942\n",
      "Epoch 3/10\n",
      "255/255 [==============================] - 3s 13ms/step - loss: 0.5600 - accuracy: 0.7766 - val_loss: 1.2635 - val_accuracy: 0.4718\n",
      "Epoch 4/10\n",
      "255/255 [==============================] - 3s 13ms/step - loss: 0.4549 - accuracy: 0.8204 - val_loss: 1.2663 - val_accuracy: 0.5213\n",
      "Epoch 5/10\n",
      "255/255 [==============================] - 3s 13ms/step - loss: 0.3832 - accuracy: 0.8496 - val_loss: 1.1546 - val_accuracy: 0.5856\n",
      "Epoch 6/10\n",
      "255/255 [==============================] - 3s 13ms/step - loss: 0.3290 - accuracy: 0.8750 - val_loss: 1.3020 - val_accuracy: 0.5748\n",
      "Epoch 7/10\n",
      "255/255 [==============================] - 3s 13ms/step - loss: 0.2841 - accuracy: 0.8927 - val_loss: 1.3284 - val_accuracy: 0.5856\n",
      "Epoch 8/10\n",
      "255/255 [==============================] - 3s 13ms/step - loss: 0.2436 - accuracy: 0.9106 - val_loss: 1.3245 - val_accuracy: 0.6135\n",
      "Epoch 9/10\n",
      "255/255 [==============================] - 4s 14ms/step - loss: 0.2145 - accuracy: 0.9235 - val_loss: 1.4404 - val_accuracy: 0.6086\n",
      "Epoch 10/10\n",
      "255/255 [==============================] - 3s 13ms/step - loss: 0.1880 - accuracy: 0.9328 - val_loss: 1.4888 - val_accuracy: 0.6135\n",
      "Epoch 1/10\n",
      "214/214 [==============================] - 4s 15ms/step - loss: 0.9721 - accuracy: 0.5187 - val_loss: 1.1116 - val_accuracy: 0.5103\n",
      "Epoch 2/10\n",
      "214/214 [==============================] - 3s 13ms/step - loss: 0.7365 - accuracy: 0.6814 - val_loss: 0.8941 - val_accuracy: 0.5965\n",
      "Epoch 3/10\n",
      "214/214 [==============================] - 3s 13ms/step - loss: 0.6246 - accuracy: 0.7428 - val_loss: 1.3146 - val_accuracy: 0.4850\n",
      "Epoch 4/10\n",
      "214/214 [==============================] - 3s 13ms/step - loss: 0.5104 - accuracy: 0.8021 - val_loss: 0.8231 - val_accuracy: 0.6991\n",
      "Epoch 5/10\n",
      "214/214 [==============================] - 3s 14ms/step - loss: 0.4129 - accuracy: 0.8432 - val_loss: 1.2523 - val_accuracy: 0.5613\n",
      "Epoch 6/10\n",
      "214/214 [==============================] - 3s 13ms/step - loss: 0.3218 - accuracy: 0.8888 - val_loss: 1.5292 - val_accuracy: 0.5279\n",
      "Epoch 7/10\n",
      "214/214 [==============================] - 3s 13ms/step - loss: 0.2583 - accuracy: 0.9148 - val_loss: 1.4989 - val_accuracy: 0.5724\n",
      "Epoch 8/10\n",
      "214/214 [==============================] - 3s 14ms/step - loss: 0.1992 - accuracy: 0.9394 - val_loss: 1.5003 - val_accuracy: 0.6082\n",
      "Epoch 9/10\n",
      "214/214 [==============================] - 3s 13ms/step - loss: 0.1559 - accuracy: 0.9539 - val_loss: 1.8498 - val_accuracy: 0.5677\n",
      "Epoch 10/10\n",
      "214/214 [==============================] - 3s 15ms/step - loss: 0.1328 - accuracy: 0.9605 - val_loss: 1.7687 - val_accuracy: 0.5935\n"
     ]
    }
   ],
   "source": [
    "# Create a function to train a simple RNN model with the given feature matrix and labels\n",
    "def train_simple_rnn(X, y, num_classes, max_words, maxlen):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, 32, input_length=maxlen))\n",
    "    model.add(SimpleRNN(32))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X, y, epochs=10, batch_size=32, validation_split=0.2)\n",
    "    return model\n",
    "\n",
    "# Train simple RNN models using the sequences for the three languages\n",
    "simple_rnn_models = {}\n",
    "for lang in languages:\n",
    "    simple_rnn_models[lang] = train_simple_rnn(X_train_sequences_padded[lang], y_train_one_hot[lang], num_classes, max_words, maxlen)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 1/84 [..............................] - ETA: 1s - loss: 1.4178 - accuracy: 0.5938"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84/84 [==============================] - 0s 4ms/step - loss: 1.2780 - accuracy: 0.6272\n",
      "166/166 [==============================] - 1s 3ms/step\n",
      "Accuracy of simple RNN model for hausa: 0.6271946430206299\n",
      "Weighted F1 score of simple RNN model for hausa: 0.6471762893448432\n",
      "58/58 [==============================] - 0s 3ms/step - loss: 1.0008 - accuracy: 0.7007\n",
      "116/116 [==============================] - 1s 4ms/step\n",
      "Accuracy of simple RNN model for igbo: 0.7007061243057251\n",
      "Weighted F1 score of simple RNN model for igbo: 0.6906287986238145\n",
      "66/66 [==============================] - 0s 3ms/step - loss: 1.4945 - accuracy: 0.6321\n",
      "142/142 [==============================] - 1s 3ms/step\n",
      "Accuracy of simple RNN model for yoruba: 0.6320574283599854\n",
      "Weighted F1 score of simple RNN model for yoruba: 0.6110113738060609\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the simple RNN models using the dev data and test data and print Accuracy and weighted F1 score\n",
    "simple_rnn_scores = {}\n",
    "simple_rnn_predictions = {}\n",
    "for lang in languages:\n",
    "    simple_rnn_scores[lang] = simple_rnn_models[lang].evaluate(X_dev_sequences_padded[lang], y_dev_one_hot[lang])\n",
    "    simple_rnn_predictions[lang] = simple_rnn_models[lang].predict(X_test_sequences_padded[lang])\n",
    "    print(f\"Accuracy of simple RNN model for {lang}:\", simple_rnn_scores[lang][1])\n",
    "    print(f\"Weighted F1 score of simple RNN model for {lang}:\", metrics.f1_score(test_data[lang]['label'], np.argmax(simple_rnn_predictions[lang], axis=1), average='weighted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "89/89 [==============================] - 8s 71ms/step - loss: 1.5849 - accuracy: 0.5152 - val_loss: 1.6802 - val_accuracy: 0.5298\n",
      "Epoch 2/10\n",
      "89/89 [==============================] - 6s 68ms/step - loss: 1.0554 - accuracy: 0.6660 - val_loss: 1.3243 - val_accuracy: 0.5658\n",
      "Epoch 3/10\n",
      "89/89 [==============================] - 6s 69ms/step - loss: 0.8191 - accuracy: 0.7292 - val_loss: 1.0377 - val_accuracy: 0.6406\n",
      "Epoch 4/10\n",
      "89/89 [==============================] - 6s 69ms/step - loss: 0.6799 - accuracy: 0.7742 - val_loss: 0.8840 - val_accuracy: 0.7090\n",
      "Epoch 5/10\n",
      "89/89 [==============================] - 6s 68ms/step - loss: 0.5892 - accuracy: 0.8033 - val_loss: 1.1305 - val_accuracy: 0.6279\n",
      "Epoch 6/10\n",
      "89/89 [==============================] - 6s 68ms/step - loss: 0.5109 - accuracy: 0.8279 - val_loss: 1.1301 - val_accuracy: 0.6282\n",
      "Epoch 7/10\n",
      "89/89 [==============================] - 6s 67ms/step - loss: 0.4880 - accuracy: 0.8349 - val_loss: 1.2453 - val_accuracy: 0.6437\n",
      "Epoch 8/10\n",
      "89/89 [==============================] - 6s 69ms/step - loss: 0.4342 - accuracy: 0.8530 - val_loss: 1.3022 - val_accuracy: 0.6250\n",
      "Epoch 9/10\n",
      "89/89 [==============================] - 6s 68ms/step - loss: 0.3833 - accuracy: 0.8741 - val_loss: 1.4607 - val_accuracy: 0.6046\n",
      "Epoch 10/10\n",
      "89/89 [==============================] - 6s 67ms/step - loss: 0.3483 - accuracy: 0.8832 - val_loss: 1.2682 - val_accuracy: 0.6646\n",
      "Epoch 1/10\n",
      "64/64 [==============================] - 6s 74ms/step - loss: 1.5751 - accuracy: 0.5450 - val_loss: 1.8785 - val_accuracy: 0.0932\n",
      "Epoch 2/10\n",
      "64/64 [==============================] - 4s 70ms/step - loss: 1.0347 - accuracy: 0.7017 - val_loss: 1.5671 - val_accuracy: 0.4007\n",
      "Epoch 3/10\n",
      "64/64 [==============================] - 4s 68ms/step - loss: 0.7321 - accuracy: 0.7909 - val_loss: 1.1655 - val_accuracy: 0.5924\n",
      "Epoch 4/10\n",
      "64/64 [==============================] - 4s 68ms/step - loss: 0.5863 - accuracy: 0.8229 - val_loss: 1.4622 - val_accuracy: 0.5032\n",
      "Epoch 5/10\n",
      "64/64 [==============================] - 4s 66ms/step - loss: 0.4932 - accuracy: 0.8549 - val_loss: 1.1314 - val_accuracy: 0.6278\n",
      "Epoch 6/10\n",
      "64/64 [==============================] - 4s 70ms/step - loss: 0.4217 - accuracy: 0.8711 - val_loss: 1.1595 - val_accuracy: 0.6317\n",
      "Epoch 7/10\n",
      "64/64 [==============================] - 4s 68ms/step - loss: 0.3559 - accuracy: 0.8973 - val_loss: 1.5566 - val_accuracy: 0.5645\n",
      "Epoch 8/10\n",
      "64/64 [==============================] - 5s 71ms/step - loss: 0.3137 - accuracy: 0.9100 - val_loss: 1.7653 - val_accuracy: 0.5424\n",
      "Epoch 9/10\n",
      "64/64 [==============================] - 4s 68ms/step - loss: 0.2912 - accuracy: 0.9121 - val_loss: 1.8011 - val_accuracy: 0.5576\n",
      "Epoch 10/10\n",
      "64/64 [==============================] - 4s 68ms/step - loss: 0.2634 - accuracy: 0.9244 - val_loss: 1.6426 - val_accuracy: 0.5910\n",
      "Epoch 1/10\n",
      "54/54 [==============================] - 5s 69ms/step - loss: 1.7516 - accuracy: 0.4392 - val_loss: 1.6759 - val_accuracy: 0.4158\n",
      "Epoch 2/10\n",
      "54/54 [==============================] - 4s 67ms/step - loss: 1.2386 - accuracy: 0.6227 - val_loss: 1.3263 - val_accuracy: 0.5402\n",
      "Epoch 3/10\n",
      "54/54 [==============================] - 4s 70ms/step - loss: 0.9343 - accuracy: 0.7179 - val_loss: 1.0806 - val_accuracy: 0.6194\n",
      "Epoch 4/10\n",
      "54/54 [==============================] - 3s 64ms/step - loss: 0.7046 - accuracy: 0.8023 - val_loss: 1.2601 - val_accuracy: 0.5924\n",
      "Epoch 5/10\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 0.5440 - accuracy: 0.8571 - val_loss: 1.3315 - val_accuracy: 0.5760\n",
      "Epoch 6/10\n",
      "54/54 [==============================] - 4s 69ms/step - loss: 0.4283 - accuracy: 0.8982 - val_loss: 1.3718 - val_accuracy: 0.5836\n",
      "Epoch 7/10\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 0.3283 - accuracy: 0.9294 - val_loss: 1.4316 - val_accuracy: 0.6012\n",
      "Epoch 8/10\n",
      "54/54 [==============================] - 4s 68ms/step - loss: 0.2737 - accuracy: 0.9443 - val_loss: 1.5844 - val_accuracy: 0.6070\n",
      "Epoch 9/10\n",
      "54/54 [==============================] - 4s 69ms/step - loss: 0.2369 - accuracy: 0.9484 - val_loss: 1.9294 - val_accuracy: 0.5636\n",
      "Epoch 10/10\n",
      "54/54 [==============================] - 4s 66ms/step - loss: 0.2530 - accuracy: 0.9396 - val_loss: 2.0026 - val_accuracy: 0.5460\n",
      "84/84 [==============================] - 1s 7ms/step - loss: 1.2178 - accuracy: 0.6455\n",
      "166/166 [==============================] - 2s 8ms/step\n",
      "Accuracy of improved simple RNN model for hausa: 0.6454986929893494\n",
      "Weighted F1 score of improved simple RNN model for hausa: 0.665480017667365\n",
      "58/58 [==============================] - 0s 7ms/step - loss: 1.0580 - accuracy: 0.6985\n",
      "116/116 [==============================] - 1s 8ms/step\n",
      "Accuracy of improved simple RNN model for igbo: 0.6985334157943726\n",
      "Weighted F1 score of improved simple RNN model for igbo: 0.6976627440037287\n",
      "66/66 [==============================] - 1s 8ms/step - loss: 1.6168 - accuracy: 0.6201\n",
      "142/142 [==============================] - 1s 7ms/step\n",
      "Accuracy of improved simple RNN model for yoruba: 0.6200956702232361\n",
      "Weighted F1 score of improved simple RNN model for yoruba: 0.5985030584713956\n"
     ]
    }
   ],
   "source": [
    "# improve the simple RNN model by adding more layers\n",
    "def train_improved_simple_rnn(X, y, num_classes, max_words, maxlen):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(max_words, 128, input_length=maxlen))\n",
    "    model.add(SimpleRNN(64, kernel_regularizer=l2(0.01), return_sequences=True))\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(SimpleRNN(64))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(num_classes, activation='softmax'))\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    model.fit(X, y, epochs=10, batch_size=128, validation_split=0.2)\n",
    "    return model\n",
    "\n",
    "# Train improved simple RNN models using the sequences for the three languages\n",
    "improved_simple_rnn_models = {}\n",
    "for lang in languages:\n",
    "    improved_simple_rnn_models[lang] = train_improved_simple_rnn(X_train_sequences_padded[lang], y_train_one_hot[lang], num_classes, max_words, maxlen)\n",
    "    \n",
    "# Evaluate the improved simple RNN models using the dev data and test data and print Accuracy and weighted F1 score\n",
    "improved_simple_rnn_scores = {}\n",
    "improved_simple_rnn_predictions = {}\n",
    "for lang in languages:\n",
    "    improved_simple_rnn_scores[lang] = improved_simple_rnn_models[lang].evaluate(X_dev_sequences_padded[lang], y_dev_one_hot[lang])\n",
    "    improved_simple_rnn_predictions[lang] = improved_simple_rnn_models[lang].predict(X_test_sequences_padded[lang])\n",
    "    print(f\"Accuracy of improved simple RNN model for {lang}:\", improved_simple_rnn_scores[lang][1])\n",
    "    print(f\"Weighted F1 score of improved simple RNN model for {lang}:\", metrics.f1_score(test_data[lang]['label'], np.argmax(improved_simple_rnn_predictions[lang], axis=1), average='weighted'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
